{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69c282e2-1ad6-44cd-859b-1184b0f0bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "252460fc-a840-4806-9c7d-5a17fcd6399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement and test the utilities in support of evaluating the results\n",
    "from the region-by-region decisions and turning them into detections.\n",
    "\n",
    "All rectangles are four component lists (or tuples) giving the upper\n",
    "left and lower right corners of an axis-aligned rectangle.  For example, \n",
    "[2, 9, 12, 18] has upper left corner (2,9) and lower right (12, 18)\n",
    "\n",
    "The region predictions for an image are stored in a list of dictionaries,\n",
    "each giving the class, the activation and the bounding rectangle.\n",
    "For example,\n",
    "\n",
    "{\n",
    "    \"class\": 2,\n",
    "    \"a\":  0.67,\n",
    "    \"rectangle\": (18, 14, 50, 75)\n",
    "}\n",
    "\n",
    "if the class is 0 this means there is no detection and the rectangle\n",
    "should be ignored.  The region predictions must be turned into the\n",
    "detection results by filtering those with class 0 and through non\n",
    "maximum supression.  The resulting regions should be considered the\n",
    "\"detections\" for the image.\n",
    "\n",
    "After this, detections should be compared to the ground truth \n",
    "\n",
    "The ground truth regions for an image are stored as a list of dictionaries. \n",
    "Each dictionary contains the region's class and bounding rectangle.\n",
    "Here is an example dictionary:\n",
    "\n",
    "{\n",
    "    \"class\":  3,\n",
    "    \"rectangle\": (15, 20, 56, 65)\n",
    "}\n",
    "\n",
    "Class 0 will not appear in the ground truth.  \n",
    "\"\"\"\n",
    "\n",
    "def area(rect):\n",
    "    h = rect[3] - rect[1]\n",
    "    w = rect[2] - rect[0]\n",
    "    return h * w\n",
    "\n",
    "\n",
    "def iou(rect1, rect2):\n",
    "    \"\"\"\n",
    "    Input: two rectangles\n",
    "    Output: IOU value, which should be 0 if the rectangles do not overlap.\n",
    "    \"\"\" \n",
    "    x0, y0, x1, y1 = rect1\n",
    "    u0, v0, u1, v1 = rect2\n",
    "    ir = (max(x0, u0), max(y0, v0), min(x1, u1), min(y1, v1))\n",
    "    if ir[0] >= ir[2] or ir[1] >= ir[3]:\n",
    "        return 0\n",
    "    else:\n",
    "        return area(ir) / (area(rect1) + area(rect2) - area(ir))\n",
    "\n",
    "\n",
    "def predictions_to_detections(predictions, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Input: List of region predictions\n",
    "\n",
    "    Output: List of region predictions that are considered to be\n",
    "    detection results. These are ordered by activation with all class\n",
    "    0 predictions eliminated, and non-maximum suppression\n",
    "    applied using the standard greedy algorithm.\n",
    "    \"\"\"\n",
    "    predictions = [p for p in predictions if p['class'] != 0]\n",
    "\n",
    "    # Sort by activation in descending order\n",
    "    predictions.sort(key=lambda item: item['a'], reverse=True)\n",
    "\n",
    "    detections = []  # Final selected detections\n",
    "    detected_classes = set()  # Track detected classes\n",
    "\n",
    "    while predictions:\n",
    "        # Pick the highest-confidence prediction\n",
    "        current = predictions.pop(0)\n",
    "\n",
    "        # Skip if this class is already detected\n",
    "        if current['class'] in detected_classes:\n",
    "            continue\n",
    "\n",
    "        # Add the selected box to final detections\n",
    "        detections.append(current)\n",
    "        detected_classes.add(current['class'])\n",
    "\n",
    "        # Remove overlapping boxes of the same class (NMS)\n",
    "        predictions = [p for p in predictions \n",
    "                       if p['class'] != current['class'] or iou(p['rectangle'], current['rectangle']) < iou_threshold]\n",
    "\n",
    "    return detections\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(detections, gt_detections, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    1. The detections returned by the predictions_to_detections function\n",
    "    2. The list of ground truth regions, and\n",
    "    3. The IOU threshold\n",
    "\n",
    "    The calculation must compare each detection region to the ground\n",
    "    truth detection regions to determine which are correct and which\n",
    "    are incorrect.  Finally, it must compute the average precision for\n",
    "    up to n detections.\n",
    "\n",
    "    Returns:\n",
    "    list of correct detections,\n",
    "    list of incorrect detections,\n",
    "    list of ground truth regions that are missed,\n",
    "    AP@n value.\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    gtd_num = len(gt_detections)\n",
    "    b = []\n",
    "    correct_dets = []\n",
    "    incorrect_dets = []\n",
    "\n",
    "    # Precompute IOUs and store the best match\n",
    "    for d in detections:\n",
    "        index, g, highest_iou = None, None, 0\n",
    "\n",
    "        # Only compare if classes match\n",
    "        for i, gt in enumerate(gt_detections):\n",
    "            if d['class'] == gt['class']:\n",
    "                curr_iou = iou(d['rectangle'], gt['rectangle'])\n",
    "                if curr_iou > highest_iou:\n",
    "                    highest_iou, index, g = curr_iou, i, gt\n",
    "\n",
    "        # Check if the best IOU exceeds threshold\n",
    "        if g is None or highest_iou < iou_threshold:\n",
    "            b.append(0)\n",
    "            incorrect_dets.append(d)\n",
    "        else:\n",
    "            b.append(1)\n",
    "            correct_dets.append(d)\n",
    "            del gt_detections[index]  # Remove matched detection\n",
    "\n",
    "    # Missed detections are the ones still in gt_detections\n",
    "    missed = gt_detections\n",
    "\n",
    "\n",
    "    cumsum = np.cumsum( b )\n",
    "    b = np.array(b)\n",
    "    precision = cumsum/np.arange(1, len(b)+1)\n",
    "\n",
    "    recall = cumsum / gtd_num\n",
    "\n",
    "    recall_levels = np.linspace(0,1,11)\n",
    "    max_precision_at_recall = []\n",
    "\n",
    "    for recall_level in recall_levels:\n",
    "        valid_precisions = precision[recall >= recall_level]  # Filtered precision values\n",
    "\n",
    "        # Check if there are valid precision values before calling np.max()\n",
    "        max_precision = np.max(valid_precisions) if valid_precisions.size > 0 else 0\n",
    "        \n",
    "        max_precision_at_recall.append(max_precision)\n",
    "        \n",
    "    AP_n = np.mean(max_precision_at_recall)\n",
    "\n",
    "    return correct_dets, incorrect_dets, missed, AP_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01758bd3-d4ee-4956-97e0-800e0946e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou for (0, 5, 11, 15) (2, 9, 12, 18) is 0.37\n",
      "iou for (2, -3, 11, 4) (2, 9, 12, 18) is 0.00\n",
      "iou for (3, 12, 9, 15) (2, 9, 12, 18) is 0.20\n"
     ]
    }
   ],
   "source": [
    "def test_iou():\n",
    "    \"\"\"\n",
    "    Use this function for you own testing of your IOU function\n",
    "    \"\"\"\n",
    "    # should be .370\n",
    "    rect1 = (0, 5, 11, 15)\n",
    "    rect2 = (2, 9, 12, 18)\n",
    "    res = iou(rect1, rect2)\n",
    "    print(f\"iou for {rect1} {rect2} is {res:1.2f}\")\n",
    "\n",
    "    # should be 0\n",
    "    rect1 = (2, -3, 11, 4)\n",
    "    res = iou(rect1, rect2)\n",
    "    print(f\"iou for {rect1} {rect2} is {res:1.2f}\")\n",
    "\n",
    "    # should be 0.2\n",
    "    rect1 = (3, 12, 9, 15)\n",
    "    res = iou(rect1, rect2)\n",
    "    print(f\"iou for {rect1} {rect2} is {res:1.2f}\")\n",
    "\n",
    "test_iou()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849a3d77-2858-407b-a7e4-9337d4621cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluation_code(in_json_file):\n",
    "    with open(in_json_file, \"r\") as in_fp:\n",
    "        data = json.load(in_fp)\n",
    "    \n",
    "    region_predictions = data[\"region_predictions\"]\n",
    "    gt_detections = data[\"gt_detections\"]\n",
    "\n",
    "    detections = predictions_to_detections(region_predictions)\n",
    "    print(f\"DETECTIONS: count = {len(detections)}\")\n",
    "    if len(detections) >= 2:\n",
    "        print(f\"DETECTIONS: first activation {detections[0]['a']:.2f}\" )\n",
    "        print(f\"DETECTIONS: last activation {detections[-1]['a']:.2f}\")\n",
    "    elif len(detections) == 1:\n",
    "        print(f\"DETECTIONS: only activation {detections[0]['a']:.2f}\")\n",
    "    else:\n",
    "        print(f\"DETECTIONS: no activations\")\n",
    "\n",
    "    correct, incorrect, missed, ap = evaluate(detections, gt_detections)\n",
    "\n",
    "    print(f\"AP: num correct {len(correct)}\")\n",
    "    if len(correct) > 0:\n",
    "        print(f\"AP: first correct activation {correct[0]['a']:.2f}\")\n",
    "\n",
    "    print(f\"AP: num incorrect {len(incorrect)}\")\n",
    "    if len(incorrect) > 0:\n",
    "        print(f\"AP: first incorrect activation {incorrect[0]['a']:.2f}\")\n",
    "\n",
    "    print(f\"AP: num ground truth missed {len(missed)}\")\n",
    "    print(f\"AP: final AP value {ap:1.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58701693-1bf6-41fa-a939-17b684db888e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECTIONS: count = 2\n",
      "DETECTIONS: first activation 0.90\n",
      "DETECTIONS: last activation 0.70\n",
      "AP: num correct 1\n",
      "AP: first correct activation 0.90\n",
      "AP: num incorrect 1\n",
      "AP: first incorrect activation 0.70\n",
      "AP: num ground truth missed 2\n",
      "AP: final AP value 0.364\n"
     ]
    }
   ],
   "source": [
    "test_evaluation_code('eval_test1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa09e592-f4ec-47e0-85b0-e26c89a5df86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECTIONS: count = 5\n",
      "DETECTIONS: first activation 0.94\n",
      "DETECTIONS: last activation 0.55\n",
      "AP: num correct 4\n",
      "AP: first correct activation 0.90\n",
      "AP: num incorrect 1\n",
      "AP: first incorrect activation 0.94\n",
      "AP: num ground truth missed 1\n",
      "AP: final AP value 0.655\n"
     ]
    }
   ],
   "source": [
    "test_evaluation_code('eval_test2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf04ca1-27a9-45aa-9223-d5b1b5103ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECTIONS: count = 1\n",
      "DETECTIONS: only activation 0.94\n",
      "AP: num correct 0\n",
      "AP: num incorrect 1\n",
      "AP: first incorrect activation 0.94\n",
      "AP: num ground truth missed 1\n",
      "AP: final AP value 0.000\n"
     ]
    }
   ],
   "source": [
    "test_evaluation_code('eval_test3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e48c2d4a-bee7-483c-a33b-785a99ff5836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECTIONS: count = 11\n",
      "DETECTIONS: first activation 0.89\n",
      "DETECTIONS: last activation 0.65\n",
      "AP: num correct 10\n",
      "AP: first correct activation 0.89\n",
      "AP: num incorrect 1\n",
      "AP: first incorrect activation 0.88\n",
      "AP: num ground truth missed 1\n",
      "AP: final AP value 0.835\n"
     ]
    }
   ],
   "source": [
    "test_evaluation_code('eval_test4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d7edc7-531b-42c0-b5d3-95413118731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Skeleton model class. You will have to implement the classification and regression layers,\n",
    "along with the forward method.\n",
    "'''\n",
    "\n",
    "class RCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RCNN, self).__init__()\n",
    "\n",
    "        # Pretrained backbone. If you are on the cci machine then this will not be able to automatically download\n",
    "        #  the pretrained weights. You will have to download them locally then copy them over.\n",
    "        #  During the local download it should tell you where torch is downloading the weights to, then copy them to \n",
    "        #  ~/.cache/torch/checkpoints/ on the supercomputer.\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Remove the last fc layer of the pretrained network.\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        # Freeze backbone weights. \n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # TODO: Implement the fully connected layers for classification and regression.\n",
    "        feature_dim = 512\n",
    "        # Classification layer (num_classes + 1 for 'nothing' class)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 5))\n",
    "        \n",
    "        # Regression layer (4 coordinates for each class)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 4 * 5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        class_logits = self.classifier(x)\n",
    "        bbox_reg = self.regressor(x).view(-1, self.num_classes, 4)\n",
    "        return class_logits, bbox_reg\n",
    "    \n",
    "    def compute_loss(self, class_logits, bbox_reg, gt_classes, gt_bboxs):\n",
    "        classification_loss_fn = nn.CrossEntropyLoss()\n",
    "        classification_loss = classification_loss_fn(class_logits, gt_classes)\n",
    "        \n",
    "        regression_loss = 0\n",
    "        batch_size = class_logits.size(0)\n",
    "        for i in range(batch_size):\n",
    "            gt_class = gt_classes[i]\n",
    "            if gt_class != 0:  # Only compute regression loss for non-'nothing' classes\n",
    "                predicted_bbox = bbox_reg[i, gt_class - 1]  # -1 because class 0 is 'nothing'\n",
    "                gt_bbox = gt_bboxs[i]\n",
    "                regression_loss += torch.sum((predicted_bbox - gt_bbox) ** 2)\n",
    "        \n",
    "        total_loss = classification_loss + (0.1 * regression_loss)  # Weighted sum\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee23751-959c-47ba-912a-f813932baa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "# Dictionaries mapping class labels to names.\n",
    "LABELS_TO_NAMES = {0: 'nothing',\n",
    "                   1: 'bicycle',\n",
    "                   2: 'car',\n",
    "                   3: 'motorbike',\n",
    "                   4: 'person',}\n",
    "\n",
    "\n",
    "LABELS_TO_NAMES_LARGE = {0: 'nothing',\n",
    "                         1: 'aeroplane',\n",
    "                         2: 'bicycle',\n",
    "                         3: 'bird',\n",
    "                         4: 'boat',\n",
    "                         5: 'bottle',\n",
    "                         6: 'bus',\n",
    "                         7: 'car',\n",
    "                         8: 'cat',\n",
    "                         9: 'chair',\n",
    "                         10: 'cow',\n",
    "                         11: 'diningtable',\n",
    "                         12: 'dog',\n",
    "                         13: 'horse',\n",
    "                         14: 'motorbike',\n",
    "                         15: 'person',\n",
    "                         16: 'pottedplant',\n",
    "                         17: 'sheep',\n",
    "                         18: 'sofa',\n",
    "                         19: 'train',\n",
    "                         20: 'tvmonitor'}\n",
    "\n",
    "\n",
    "class HW5Dataset(Dataset):\n",
    "    def __init__(self, data_root, json_file, candidate_region_size=224, cache_dir=None):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data_dict = json.load(f)\n",
    "\n",
    "        self.data_root = data_root\n",
    "        self.candidate_region_size = candidate_region_size\n",
    "        self.cache_dir = cache_dir  # Directory to cache cropped images\n",
    "\n",
    "        self.images = []\n",
    "        self.candidate_bboxes = torch.empty((0, 4), dtype=int)\n",
    "        self.ground_truth_bboxes = torch.empty((0, 4), dtype=int)\n",
    "        self.ground_truth_classes = torch.empty(0, dtype=int)\n",
    "\n",
    "        for key, values in data_dict.items():\n",
    "            for val in values:\n",
    "                self.images.append(key)\n",
    "                self.candidate_bboxes = torch.cat((self.candidate_bboxes, torch.tensor(val['bbox']).unsqueeze(0)))\n",
    "                self.ground_truth_bboxes = torch.cat((self.ground_truth_bboxes, torch.tensor(val['gt_bbox']).unsqueeze(0)))\n",
    "                self.ground_truth_classes = torch.cat((self.ground_truth_classes, torch.tensor(val['class']).unsqueeze(0)))\n",
    "\n",
    "        # Define transform for candidate regions (resize, tensor conversion, and normalization)\n",
    "        self.transform = transforms.Compose([transforms.Resize((candidate_region_size, candidate_region_size)),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image.\n",
    "        image_path = join(self.data_root, self.images[idx])\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Get candidate bounding box for the current image\n",
    "        candidate_bbox = self.candidate_bboxes[idx, :]\n",
    "\n",
    "        # Define a unique cache filename based on the image and bounding box\n",
    "        cache_filename = f\"{self.images[idx]}_{candidate_bbox[0]}_{candidate_bbox[1]}_{candidate_bbox[2]}_{candidate_bbox[3]}.pt\"\n",
    "        cache_filepath = join(self.cache_dir, cache_filename) if self.cache_dir else None\n",
    "\n",
    "        # If cache_dir is provided and cached cropped image exists, load it\n",
    "        if self.cache_dir and os.path.exists(cache_filepath):\n",
    "            candidate_region = torch.load(cache_filepath)\n",
    "        else:\n",
    "            # Crop the image to the candidate region\n",
    "            candidate_region = image.crop((candidate_bbox[0].item(), candidate_bbox[1].item(), candidate_bbox[2].item(), candidate_bbox[3].item()))\n",
    "\n",
    "            # Transform (resize, tensor, normalize)\n",
    "            candidate_region = self.transform(candidate_region)\n",
    "\n",
    "            # Cache the cropped region for future access\n",
    "            if self.cache_dir:\n",
    "                os.makedirs(self.cache_dir, exist_ok=True)\n",
    "                torch.save(candidate_region, cache_filepath)\n",
    "\n",
    "        # Resize and normalize the ground truth bounding box\n",
    "        gt_bbox = self.ground_truth_bboxes[idx, :]\n",
    "        x_scale = self.candidate_region_size / (candidate_bbox[2] - candidate_bbox[0])\n",
    "        y_scale = self.candidate_region_size / (candidate_bbox[3] - candidate_bbox[1])\n",
    "\n",
    "        resized_gt_x0 = (gt_bbox[0] - candidate_bbox[0]) * x_scale / self.candidate_region_size\n",
    "        resized_gt_y0 = (gt_bbox[1] - candidate_bbox[1]) * y_scale / self.candidate_region_size\n",
    "        resized_gt_x1 = (gt_bbox[2] - candidate_bbox[0]) * x_scale / self.candidate_region_size\n",
    "        resized_gt_y1 = (gt_bbox[3] - candidate_bbox[1]) * y_scale / self.candidate_region_size\n",
    "\n",
    "        resized_gt_bbox = torch.tensor([resized_gt_x0, resized_gt_y0, resized_gt_x1, resized_gt_y1])\n",
    "\n",
    "        return candidate_region, resized_gt_bbox, self.ground_truth_classes[idx]\n",
    "\n",
    "\n",
    "class HW5DatasetTest(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Test.\n",
    "    Input:\n",
    "        data_root - path to the test image directory\n",
    "        json_file - path to test.json\n",
    "    Returns:\n",
    "        image - numpy array A x B x 3 (RGB)\n",
    "        candidate_regions - NUM_CANDIDATE_REGIONS x 3 x M x M tensor\n",
    "        candidate_bboxes - all candidate bounding boxes for an image \n",
    "        ground_truth_bboxes - all ground truth bounding boxes for an image\n",
    "        ground_truth_classes - all ground truth classes for an image\n",
    "    \"\"\"\n",
    "    def __init__(self, data_root, json_file, candidate_region_size=224):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data_dict = json.load(f)\n",
    "\n",
    "        self.data_root = data_root\n",
    "\n",
    "        self.images = []\n",
    "        self.candidate_bboxes = []\n",
    "        self.ground_truth_bboxes = []\n",
    "        self.ground_truth_classes = []\n",
    "        for key, values in data_dict.items():\n",
    "            self.images.append(key)\n",
    "\n",
    "            bboxes = torch.empty((len(values['candidate_bboxes']), 4), dtype=int)\n",
    "            for i, bbox in enumerate(values['candidate_bboxes']):\n",
    "                bboxes[i, :] = torch.tensor(bbox)\n",
    "            self.candidate_bboxes.append(bboxes)\n",
    "\n",
    "            labels = torch.empty((len(values['gt_bboxes'])), dtype=int)\n",
    "            bboxes = torch.empty((len(values['gt_bboxes']), 4), dtype=int)\n",
    "            for i, bbox in enumerate(values['gt_bboxes']):\n",
    "                bboxes[i, :] = torch.tensor(bbox['bbox'])\n",
    "                labels[i] = bbox['class']\n",
    "            self.ground_truth_bboxes.append(bboxes)\n",
    "            self.ground_truth_classes.append(labels)\n",
    "\n",
    "        self.candidate_region_size = candidate_region_size\n",
    "\n",
    "        # Transform to resize, convert to tensor, and normalize.\n",
    "        self.transform = transforms.Compose([transforms.Resize((candidate_region_size, candidate_region_size)),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image.\n",
    "        image_path = join(self.data_root, self.images[idx])\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Apply transform to resize and normalize the candidate images.\n",
    "        idx_candidate_bboxes = self.candidate_bboxes[idx]\n",
    "        candidate_regions = torch.empty((len(idx_candidate_bboxes), 3, self.candidate_region_size, self.candidate_region_size))\n",
    "        for i, bbox in enumerate(idx_candidate_bboxes):\n",
    "            candidate_region = image.crop((bbox[0].item(), bbox[1].item(), bbox[2].item(), bbox[3].item()))\n",
    "            candidate_region = self.transform(candidate_region)\n",
    "            candidate_regions[i] = candidate_region\n",
    "\n",
    "        return np.array(image), candidate_regions, self.candidate_bboxes[idx], self.ground_truth_bboxes[idx], self.ground_truth_classes[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "048b955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "    train_ious, val_ious = [], []\n",
    "    \n",
    "    # Initialize lists for confusion matrices\n",
    "    all_train_preds, all_train_labels = [], []\n",
    "    all_val_preds, all_val_labels = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Training epoch {epoch+1}\")\n",
    "        \n",
    "        # Clear prediction lists at start of each epoch\n",
    "        epoch_train_preds, epoch_train_labels = [], []\n",
    "        epoch_val_preds, epoch_val_labels = [], []\n",
    "        \n",
    "        # Training phase\n",
    "        print(\".train()\")\n",
    "        model.train()\n",
    "        epoch_train_loss, epoch_train_iou = 0, []\n",
    "        \n",
    "        for batch, (images, gt_bboxes, gt_classes) in enumerate(train_loader):\n",
    "            print(\"Converting to device\")\n",
    "            images, gt_bboxes, gt_classes = images.to(device), gt_bboxes.to(device, dtype=torch.float32), gt_classes.to(device, dtype=torch.long)\n",
    "            \n",
    "            # Forward pass\n",
    "            class_logits, bbox_reg = model(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = model.compute_loss(class_logits, bbox_reg, gt_classes, gt_bboxes)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "            # Get predictions for confusion matrix\n",
    "            _, pred_classes = torch.max(class_logits, 1)\n",
    "            epoch_train_preds.extend(pred_classes.cpu().numpy())\n",
    "            epoch_train_labels.extend(gt_classes.cpu().numpy())\n",
    "            \n",
    "            # Calculate IoU for correct classifications\n",
    "            correct_mask = (pred_classes == gt_classes) & (gt_classes != 0)\n",
    "            if correct_mask.any():\n",
    "                for i in torch.where(correct_mask)[0]:\n",
    "                    pred_bbox = bbox_reg[i, gt_classes[i]-1].detach().cpu().numpy()\n",
    "                    true_bbox = gt_bboxes[i].detach().cpu().numpy()\n",
    "                    epoch_train_iou.append(iou(pred_bbox, true_bbox))\n",
    "        \n",
    "        # Validation phase\n",
    "        print(f\"Beginning validation for epoch {epoch+1}\")\n",
    "        model.eval()\n",
    "        epoch_val_loss, epoch_val_iou = 0, []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, gt_bboxes, gt_classes in val_loader:\n",
    "                images, gt_bboxes, gt_classes = images.to(device), gt_bboxes.to(device, dtype=torch.float32), gt_classes.to(device, dtype=torch.long)\n",
    "                \n",
    "                # Forward pass\n",
    "                class_logits, bbox_reg = model(images)\n",
    "                loss = model.compute_loss(class_logits, bbox_reg, gt_classes, gt_bboxes)\n",
    "                epoch_val_loss += loss.item()\n",
    "                \n",
    "                # Get predictions for confusion matrix\n",
    "                _, pred_classes = torch.max(class_logits, 1)\n",
    "                epoch_val_preds.extend(pred_classes.cpu().numpy())\n",
    "                epoch_val_labels.extend(gt_classes.cpu().numpy())\n",
    "                \n",
    "                # Calculate IoU for correct classifications\n",
    "                correct_mask = (pred_classes == gt_classes) & (gt_classes != 0)\n",
    "                if correct_mask.any():\n",
    "                    for i in torch.where(correct_mask)[0]:\n",
    "                        pred_bbox = bbox_reg[i, gt_classes[i]-1].cpu().numpy()\n",
    "                        true_bbox = gt_bboxes[i].cpu().numpy()\n",
    "                        epoch_val_iou.append(iou(pred_bbox, true_bbox))\n",
    "        \n",
    "        # Store predictions for final confusion matrix\n",
    "        all_train_preds.extend(epoch_train_preds)\n",
    "        all_train_labels.extend(epoch_train_labels)\n",
    "        all_val_preds.extend(epoch_val_preds)\n",
    "        all_val_labels.extend(epoch_val_labels)\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        avg_train_iou = np.mean(epoch_train_iou) if epoch_train_iou else 0\n",
    "        avg_val_iou = np.mean(epoch_val_iou) if epoch_val_iou else 0\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_ious.append(avg_train_iou)\n",
    "        val_ious.append(avg_val_iou)\n",
    "        \n",
    "        # Model checkpointing based on validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        # Print training and validation results for each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}')\n",
    "        print(f'Train IoU: {avg_train_iou:.4f} | Val IoU: {avg_val_iou:.4f}')\n",
    "        print('-' * 50)\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_ious, label='Train IoU')\n",
    "    plt.plot(val_ious, label='Val IoU')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Confusion matrix plotting function\n",
    "    def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "        \n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Training confusion matrix\n",
    "    train_cm = confusion_matrix(all_train_labels, all_train_preds)\n",
    "    plot_confusion_matrix(train_cm, list(LABELS_TO_NAMES.values()), 'Training Confusion Matrix')\n",
    "    \n",
    "    # Validation confusion matrix\n",
    "    val_cm = confusion_matrix(all_val_labels, all_val_preds)\n",
    "    plot_confusion_matrix(val_cm, list(LABELS_TO_NAMES.values()), 'Validation Confusion Matrix')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a3d39cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(image, detections, gt_detections, class_names):\n",
    "    \"\"\"\n",
    "    Visualize detections and ground truth on an image\n",
    "    :param image: PIL Image or numpy array\n",
    "    :param detections: List of detection dictionaries with 'rectangle', 'class', 'a'\n",
    "    :param gt_detections: List of ground truth dictionaries with 'rectangle', 'class'\n",
    "    :param class_names: Dictionary mapping class IDs to names\n",
    "    \"\"\"\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Draw ground truth boxes (yellow)\n",
    "    for gt in gt_detections:\n",
    "        rect = gt['rectangle']\n",
    "        box = patches.Rectangle((rect[0], rect[1]), rect[2]-rect[0], rect[3]-rect[1],\n",
    "                               linewidth=2, edgecolor='y', facecolor='none')\n",
    "        ax.add_patch(box)\n",
    "        plt.text(rect[0], rect[1]-5, f\"GT: {class_names[gt]['class']}\",\n",
    "                color='y', fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "    \n",
    "    # Draw detections (green=correct, red=incorrect)\n",
    "    for det in detections:\n",
    "        rect = det['rectangle']\n",
    "        class_id = det['class']\n",
    "        \n",
    "        # Check if detection matches any ground truth\n",
    "        is_correct = False\n",
    "        for gt in gt_detections:\n",
    "            if gt['class'] == class_id and iou(rect, gt['rectangle']) >= 0.5:\n",
    "                is_correct = True\n",
    "                break\n",
    "        \n",
    "        color = 'g' if is_correct else 'r'\n",
    "        box = patches.Rectangle((rect[0], rect[1]), rect[2]-rect[0], rect[3]-rect[1],\n",
    "                               linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(box)\n",
    "        plt.text(rect[0], rect[1]-5, f\"{class_names[class_id]} ({det['a']:.2f})\",\n",
    "                color=color, fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, test_loader, class_names, device='cuda'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    aps = []\n",
    "    class_stats = {class_id: {'tp': 0, 'fp': 0, 'fn': 0} \n",
    "                  for class_id in class_names if class_id != 0}\n",
    "    \n",
    "    # Specific test images to visualize\n",
    "    visualize_indices = [0, 15, 27, 29, 38, 74, 77, 87, 92, 145]  # 0-based\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (image, candidate_regions, candidate_bboxes, gt_bboxes, gt_classes) in enumerate(test_loader):\n",
    "            if idx not in visualize_indices:\n",
    "                continue\n",
    "            \n",
    "            # Move data to device\n",
    "            candidate_regions = candidate_regions.to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            class_logits, bbox_reg = model(candidate_regions)\n",
    "            \n",
    "            # Convert to detections\n",
    "            _, pred_classes = torch.max(class_logits, 1)\n",
    "            pred_scores = torch.softmax(class_logits, 1).max(1)[0]\n",
    "            \n",
    "            # Prepare detections for NMS\n",
    "            detections = []\n",
    "            for i in range(len(pred_classes)):\n",
    "                if pred_classes[i] != 0:  # Skip 'nothing' class\n",
    "                    class_id = pred_classes[i].item()\n",
    "                    score = pred_scores[i].item()\n",
    "                    # Convert bbox from relative to absolute coordinates\n",
    "                    rel_bbox = bbox_reg[i, class_id-1].cpu().numpy()  # -1 because class 0 is nothing\n",
    "                    abs_bbox = [\n",
    "                        rel_bbox[0] * (candidate_bboxes[i][2] - candidate_bboxes[i][0]) + candidate_bboxes[i][0],\n",
    "                        rel_bbox[1] * (candidate_bboxes[i][3] - candidate_bboxes[i][1]) + candidate_bboxes[i][1],\n",
    "                        rel_bbox[2] * (candidate_bboxes[i][2] - candidate_bboxes[i][0]) + candidate_bboxes[i][0],\n",
    "                        rel_bbox[3] * (candidate_bboxes[i][3] - candidate_bboxes[i][1]) + candidate_bboxes[i][1]\n",
    "                    ]\n",
    "                    detections.append({\n",
    "                        'class': class_id,\n",
    "                        'a': score,\n",
    "                        'rectangle': abs_bbox\n",
    "                    })\n",
    "            \n",
    "            # Apply NMS\n",
    "            final_detections = predictions_to_detections(detections)\n",
    "            \n",
    "            # Prepare ground truth\n",
    "            gt_detections = [{\n",
    "                'class': gt_classes[i].item(),\n",
    "                'rectangle': gt_bboxes[i].tolist()\n",
    "            } for i in range(len(gt_classes))]\n",
    "            \n",
    "            # Evaluate\n",
    "            correct_dets, incorrect_dets, missed_dets, ap = evaluate(final_detections, gt_detections)\n",
    "            aps.append(ap)\n",
    "            \n",
    "            # Update class statistics\n",
    "            for det in correct_dets:\n",
    "                class_stats[det['class']]['tp'] += 1\n",
    "            for det in incorrect_dets:\n",
    "                class_stats[det['class']]['fp'] += 1\n",
    "            for gt in missed_dets:\n",
    "                class_stats[gt['class']]['fn'] += 1\n",
    "            \n",
    "            # Visualize\n",
    "            print(f\"Image {idx+1} AP: {ap:.4f}\")\n",
    "            visualize_detections(image[0], final_detections, gt_detections, class_names)\n",
    "    \n",
    "    # Calculate mAP and class statistics\n",
    "    mAP = np.mean(aps)\n",
    "    print(f\"\\nMean Average Precision (mAP): {mAP:.4f}\")\n",
    "    \n",
    "    print(\"\\nClass-wise Statistics:\")\n",
    "    for class_id, stats in class_stats.items():\n",
    "        precision = stats['tp'] / (stats['tp'] + stats['fp']) if (stats['tp'] + stats['fp']) > 0 else 0\n",
    "        recall = stats['tp'] / (stats['tp'] + stats['fn']) if (stats['tp'] + stats['fn']) > 0 else 0\n",
    "        print(f\"{class_names[class_id]}:\")\n",
    "        print(f\"  Precision: {precision:.2%}\")\n",
    "        print(f\"  Recall: {recall:.2%}\")\n",
    "        print(f\"  TP: {stats['tp']}, FP: {stats['fp']}, FN: {stats['fn']}\")\n",
    "    \n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data sets\n",
      "Loaded data sets into DataLoader\n",
      "Training epoch 1\n",
      ".train()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = HW5Dataset(\"../../../../Documents/hw5_2024_data/train\", \"train.json\")\n",
    "    val_dataset = HW5Dataset(\"../../../../Documents/hw5_2024_data/valid\", \"valid.json\")\n",
    "    test_dataset = HW5DatasetTest(\"../../../../Documents/hw5_2024_data/test\", \"test.json\")\n",
    "\n",
    "    print(\"Loaded data sets\")\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "    \n",
    "    print(\"Loaded data sets into DataLoader\")\n",
    "    \n",
    "    # Initialize and train model\n",
    "    model = RCNN()  # 4 classes + nothing\n",
    "    model = train_model(model, train_loader, val_loader, num_epochs=10, device=device)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    evaluate_model(model, test_loader, LABELS_TO_NAMES, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
